#!/usr/bin/env python3
"""
train_enhanced.py â€” Improved commit classifier training script.

Improvements over baseline:
1. Feature Engineering:
   - Diff Similarity: Jaccard index between added and deleted tokens (targets 'refactor').
   - Path Tokens: Tokenizes file paths from diff headers (targets 'test', 'ci', 'docs').
2. Model:
   - Replaced LogisticRegression with LinearSVC (better for high-dimensional sparse text data).
3. Data Handling:
   - Parses paths directly from diff_text (no need to change miner.py schema).

Usage:
  python train_enhanced.py --data datasets/combined_train.jsonl --model out/model_v2.joblib
"""
import argparse
import json
import re
import sys
from pathlib import Path
from typing import List, Set

import joblib
import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC

# å˜—è©¦åŒ¯å…¥ ONNX ç›¸é—œå¥—ä»¶ (éå¿…è¦)
try:
    from skl2onnx import to_onnx
    from skl2onnx.common.data_types import StringTensorType, FloatTensorType
    HAS_ONNX = True
except ImportError:
    HAS_ONNX = False


# -----------------------------------------------------------------------------
# Custom Feature Extractors
# -----------------------------------------------------------------------------

class DiffSimilarityExtractor(BaseEstimator, TransformerMixin):
    """
    è¨ˆç®— Diff ä¸­æ–°å¢éƒ¨åˆ†èˆ‡åˆªé™¤éƒ¨åˆ†çš„æ–‡å­— Jaccard Similarityã€‚
    Refactor é€šå¸¸å…·æœ‰è¼ƒé«˜çš„ç›¸ä¼¼åº¦ï¼ˆæ¬ç§»ä»£ç¢¼ã€æ”¹åï¼‰ï¼Œè€Œ Feat é€šå¸¸å¾ˆä½ã€‚
    """
    def __init__(self):
        self.token_pattern = re.compile(r'(?u)\b\w+\b')

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        # X is a list/series of diff_text strings
        scores = []
        for diff in X:
            scores.append(self._compute_jaccard(str(diff)))
        return np.array(scores).reshape(-1, 1)

    def _compute_jaccard(self, diff_text: str) -> float:
        if not diff_text:
            return 0.0
        
        adds_tokens = set()
        dels_tokens = set()
        
        for line in diff_text.splitlines():
            # è·³é header
            if line.startswith('+++') or line.startswith('---'):
                continue
            
            # ç°¡å–®åˆ†è©
            if line.startswith('+'):
                adds_tokens.update(self.token_pattern.findall(line[1:].lower()))
            elif line.startswith('-'):
                dels_tokens.update(self.token_pattern.findall(line[1:].lower()))
        
        if not adds_tokens and not dels_tokens:
            return 0.0
            
        intersection = len(adds_tokens & dels_tokens)
        union = len(adds_tokens | dels_tokens)
        
        return intersection / union if union > 0 else 0.0


class PathTokenExtractor(BaseEstimator, TransformerMixin):
    """
    å¾ Diff Text ä¸­æå–æª”æ¡ˆè·¯å¾‘ï¼Œä¸¦é€²è¡Œåˆ†è©ã€‚
    ä¾‹å¦‚: "diff --git a/src/auth/login.spec.ts" -> "src auth login spec ts"
    é€™èƒ½æ•æ‰ 'spec', 'test', 'github', 'workflows' ç­‰é—œéµå­—ã€‚
    """
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        paths_list = []
        for diff in X:
            paths_list.append(self._extract_path_tokens(str(diff)))
        return paths_list

    def _extract_path_tokens(self, diff_text: str) -> str:
        # æŠ“å– diff --git a/path/to/file b/...
        # æˆ–è€… +++ b/path/to/file
        tokens = set()
        
        # ç°¡å–®ç­–ç•¥ï¼šæŠ“å– +++ b/ ä¹‹å¾Œçš„è·¯å¾‘
        # æˆ–æ˜¯ diff --git a/ ä¹‹å¾Œçš„è·¯å¾‘
        # é€™è£¡ç”¨ä¸€å€‹ç°¡å–®çš„ regex ä¾†æŠ“å–å¯èƒ½çš„è·¯å¾‘å­—ä¸²
        path_matches = re.findall(r'^\+\+\+ b/(.+)$', diff_text, re.MULTILINE)
        if not path_matches:
            # å˜—è©¦æŠ“ diff --git
            path_matches = re.findall(r'^diff --git a/.+ b/(.+)$', diff_text, re.MULTILINE)
            
        for path in path_matches:
            # å°‡è·¯å¾‘æ‹†è§£ç‚º token: src/utils/foo.py -> src, utils, foo, py
            parts = re.split(r'[/\-_.]', path)
            for p in parts:
                if len(p) > 2: # éæ¿¾å¤ªçŸ­çš„
                    tokens.add(p.lower())
                    
        return " ".join(tokens)


# -----------------------------------------------------------------------------
# Main Pipeline
# -----------------------------------------------------------------------------

def load_data(data_path: str):
    print(f"ğŸ“‚ Loading data from {data_path}...")
    data = []
    path = Path(data_path)
    
    # æ”¯æ´è®€å–å–®ä¸€ jsonl æˆ–è³‡æ–™å¤¾å…§æ‰€æœ‰ jsonl
    files = [path] if path.is_file() else list(path.glob("*.jsonl"))
    
    for p in files:
        with open(p, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    try:
                        data.append(json.loads(line))
                    except json.JSONDecodeError:
                        continue
    return pd.DataFrame(data)

def main():
    parser = argparse.ArgumentParser(description="Train enhanced commit classifier")
    parser.add_argument("--data", required=True, help="Path to JSONL dataset(s)")
    parser.add_argument("--model", default="out/model_v2.joblib", help="Output model path")
    parser.add_argument("--onnx", default="out/model_v2.onnx", help="Output ONNX path")
    parser.add_argument("--max_diff_len", type=int, default=20000, help="Truncate diff text")
    args = parser.parse_args()

    # 1. è¼‰å…¥è³‡æ–™
    df = load_data(args.data)
    if df.empty:
        print("âŒ No data found.")
        return

    # ç°¡å–®æ¸…ç†
    df['diff_text'] = df['diff_text'].fillna('')
    df['diff_text'] = df['diff_text'].apply(lambda x: x[:args.max_diff_len])
    
    # è¨ˆç®—åŸºç¤æ•¸å€¼ç‰¹å¾µ
    for col in ['files_changed', 'additions', 'deletions']:
        df[col] = pd.to_numeric(df.get(col, 0), errors='coerce').fillna(0)
    
    # å¢åŠ ç‰¹å¾µ: Add/Del Ratio (å° feat/fix æœ‰ç”¨)
    df['add_del_ratio'] = df['additions'] / (df['deletions'] + 1)

    print(f"ğŸ“Š Training on {len(df)} samples...")
    print(f"   Labels: {df['label'].unique()}")

    X = df[['diff_text', 'files_changed', 'additions', 'deletions', 'add_del_ratio']]
    y = df['label']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.1, random_state=42, stratify=y
    )

    # 2. å®šç¾© Pipeline
    
    # ç‰¹å¾µå·¥ç¨‹çµ„åˆ
    preprocessor = ColumnTransformer(
        transformers=[
            # A. Diff å…§å®¹æœ¬èº«çš„æ–‡å­—ç‰¹å¾µ (TF-IDF)
            ('diff_tfidf', TfidfVectorizer(max_features=10000, stop_words='english'), 'diff_text'),
            
            # B. è·¯å¾‘é—œéµå­— (Path Tokens) - å¾ diff_text æå–
            ('path_bow', Pipeline([
                ('extractor', PathTokenExtractor()),
                ('vect', CountVectorizer(max_features=2000, binary=True))
            ]), 'diff_text'),
            
            # C. Diff ç›¸ä¼¼åº¦ (Jaccard) - è§£æ±º Refactor
            ('diff_sim', DiffSimilarityExtractor(), 'diff_text'),
            
            # D. æ•¸å€¼ç‰¹å¾µæ¨™æº–åŒ–
            ('numeric', StandardScaler(), ['files_changed', 'additions', 'deletions', 'add_del_ratio']),
        ],
        remainder='drop'
    )

    # æ¨¡å‹: LinearSVC (æ¯” LR æ›´é©åˆç¨€ç–é«˜ç¶­ç‰¹å¾µï¼Œä¸”è¼•é‡)
    clf = LinearSVC(class_weight='balanced', random_state=42, max_iter=5000)

    model = Pipeline([
        ('preprocessor', preprocessor),
        ('clf', clf)
    ])

    # 3. è¨“ç·´
    print("ğŸš€ Training model...")
    model.fit(X_train, y_train)

    # 4. è©•ä¼°
    print("âš–ï¸  Evaluating...")
    y_pred = model.predict(X_test)
    print("\n" + classification_report(y_test, y_pred))

    # é¡¯ç¤º Confusion Matrix (Text)
    labels = sorted(model.classes_)
    cm = confusion_matrix(y_test, y_pred, labels=labels)
    print("\nConfusion Matrix:")
    print(pd.DataFrame(cm, index=labels, columns=labels))

    # 5. å„²å­˜
    Path(args.model).parent.mkdir(parents=True, exist_ok=True)
    joblib.dump(model, args.model)
    print(f"\nğŸ’¾ Model saved to {args.model}")
    
    # å„²å­˜ labels å°ç…§è¡¨
    with open(Path(args.model).parent / 'labels.txt', 'w') as f:
        f.write('\n'.join(labels))

    # 6. åŒ¯å‡º ONNX (Optional)
    if HAS_ONNX and args.onnx:
        print("ğŸ“¦ Exporting to ONNX...")
        try:
            # å®šç¾©è¼¸å…¥å‹åˆ¥
            # æ³¨æ„: é€™è£¡å¿…é ˆèˆ‡ ColumnTransformer çš„è¼¸å…¥å°é½Š
            # é›–ç„¶æˆ‘å€‘å‚³å…¥ DataFrameï¼Œä½†åœ¨ ONNX ä¸­é€šå¸¸å®šç¾©ç‚ºå¹¾å€‹ Tensor
            initial_types = [
                ('diff_text', StringTensorType([None, 1])),
                ('files_changed', FloatTensorType([None, 1])),
                ('additions', FloatTensorType([None, 1])),
                ('deletions', FloatTensorType([None, 1])),
                ('add_del_ratio', FloatTensorType([None, 1])),
            ]
            
            # ONNX export å°æ–¼è‡ªå®šç¾© Transformer (DiffSimilarityExtractor) å¯èƒ½æœƒé‡åˆ°å›°é›£
            # å› ç‚ºå®ƒåŒ…å« Python codeã€‚
            # ç‚ºäº†è®“å®ƒèƒ½è¢« exportï¼Œé€šå¸¸éœ€è¦è¨»å†Š custom converterï¼Œé€™æ¯”è¼ƒè¤‡é›œã€‚
            # å¦‚æœåªæ˜¯è¦åœ¨ Python ç’°å¢ƒç”¨ï¼Œjoblib å°±å¤ äº†ã€‚
            # å¦‚æœä¸€å®šè¦ ONNXï¼Œé€™è£¡å¯èƒ½éœ€è¦ç°¡åŒ–ç‰¹å¾µæˆ–å¯« converterã€‚
            # ç‚ºäº†ä¿æŒè…³æœ¬ç°¡å–®ï¼Œé€™è£¡å…ˆåšä¸€å€‹ try-catch æé†’ã€‚
            
            onx = to_onnx(model, X_train[:1], options={id(clf): {'zipmap': False}})
            with open(args.onnx, "wb") as f:
                f.write(onx.SerializeToString())
            print(f"   ONNX saved to {args.onnx}")
        except Exception as e:
            print(f"âš ï¸  ONNX export skipped/failed (likely due to custom transformers): {e}")
            print("   (To fix this, you'd need to register custom ONNX converters or rely on Python runtime)")

if __name__ == '__main__':
    main()